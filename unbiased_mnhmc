
#!/usr/bin/env python
"""
unbiased_multinomial_hmc.py

This module implements a coupled multinomial Hamiltonian Monte Carlo (HMC) kernel.
The update mechanism is as follows:

  - At each update, the kernel performs a coupled update by randomly choosing
    (via a coin toss) between a coupled multinomial HMC update and a coupled Metropolis-Hastings (MH) update.
  - The multinomial HMC update simulates a full trajectory using Hamiltonian dynamics,
    following the procedure in Section 2.2 of the attached paper:
      * A number L_f is uniformly sampled from {0, 1, ..., L} as the number of forward leapfrog steps.
      * The number of backward steps is then L - L_f.
      * The forward trajectory is obtained by integrating from (q0, p0) for L_f steps;
        the backward trajectory is obtained by integrating from (q0, -p0) for L - L_f steps.
      * The backward trajectory (excluding the duplicate q0) is reversed and concatenated with the forward trajectory.
  - When the current time step t is less than a specified lag, only the X-chain is updated (Y stays unchanged)
    so that both chains use identical marginal updates during the lag period.
  - With probability mix_prob a coupled MH update is performed using the external MH kernel,
    called coupledRWMHKernel (imported from coupled_mh_kernel.py).

All comments are in English.
"""

import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np

# Import the externally defined coupled MH kernel.
from coupled_mh_kernel import CoupledMetropolisHastingsKernel  # This is our coupledRWMHKernel

tfd = tfp.distributions

def leapfrog_integrator(q, p, step_size, target_log_prob_fn):
    """
    Performs one step of the leapfrog integration.

    Args:
        q: Tensor of shape [batch, d] for positions.
        p: Tensor of shape [batch, d] for momenta.
        step_size: A float integration step size.
        target_log_prob_fn: A callable mapping [batch, d] to log probability.

    Returns:
        A tuple (q_new, p_new) with updated positions and momenta.
    """
    with tf.GradientTape() as tape:
        tape.watch(q)
        logp = target_log_prob_fn(q)
    grad_U = -tape.gradient(logp, q)
    p_half = p - 0.5 * step_size * grad_U
    q_new = q + step_size * p_half
    with tf.GradientTape() as tape:
        tape.watch(q_new)
        logp_new = target_log_prob_fn(q_new)
    grad_U_new = -tape.gradient(logp_new, q_new)
    p_new = p_half - 0.5 * step_size * grad_U_new
    return q_new, p_new

def simulate_trajectory(q0, p0, L, step_size, target_log_prob_fn):
    """
    Simulates a full trajectory for multinomial HMC.

    Procedure:
      1. Sample L_f uniformly in {0, 1, ..., L} as the number of forward steps.
         Let L_b = L - L_f be the number of backward steps.
      2. Perform forward integration starting from (q0, p0) for L_f steps.
      3. Perform backward integration starting from (q0, -p0) for L_b steps.
      4. Reverse the backward trajectory (excluding the starting state) and concatenate it with the forward trajectory.
         This forms a complete trajectory of length L+1.

    Args:
        q0: Tensor of shape [batch, d] for the initial positions.
        p0: Tensor of shape [batch, d] for the initial momenta.
        L: Integer, maximum number of leapfrog steps.
        step_size: Float step size.
        target_log_prob_fn: Callable mapping [batch, d] -> log probability.

    Returns:
        A tuple (traj_q, traj_p) where:
          - traj_q is a tensor of shape [L+1, batch, d] containing the trajectory positions.
          - traj_p is a tensor of shape [L+1, batch, d] containing the trajectory momenta.
    """
    batch = tf.shape(q0)[0]
    d = tf.shape(q0)[1]
    # Sample number of forward steps uniformly from {0,1,...,L}
    L_f = tf.random.uniform([], minval=0, maxval=L+1, dtype=tf.int32)
    L_b = L - L_f

    # Forward integration (using positive momentum)
    forward_q_array = tf.TensorArray(dtype=q0.dtype, size=L_f + 1)
    forward_p_array = tf.TensorArray(dtype=p0.dtype, size=L_f + 1)
    forward_q_array = forward_q_array.write(0, q0)
    forward_p_array = forward_p_array.write(0, p0)
    q_curr = q0
    p_curr = p0
    for i in tf.range(1, L_f + 1):
        q_curr, p_curr = leapfrog_integrator(q_curr, p_curr, step_size, target_log_prob_fn)
        forward_q_array = forward_q_array.write(i, q_curr)
        forward_p_array = forward_p_array.write(i, p_curr)
    forward_q = forward_q_array.stack()  # shape: [L_f+1, batch, d]
    forward_p = forward_p_array.stack()    # shape: [L_f+1, batch, d]

    # Backward integration (using negative momentum)
    backward_q_array = tf.TensorArray(dtype=q0.dtype, size=L_b + 1)
    backward_p_array = tf.TensorArray(dtype=p0.dtype, size=L_b + 1)
    backward_q_array = backward_q_array.write(0, q0)
    backward_p_array = backward_p_array.write(0, -p0)
    q_curr_b = q0
    p_curr_b = -p0
    for i in tf.range(1, L_b + 1):
        q_curr_b, p_curr_b = leapfrog_integrator(q_curr_b, p_curr_b, step_size, target_log_prob_fn)
        backward_q_array = backward_q_array.write(i, q_curr_b)
        backward_p_array = backward_p_array.write(i, p_curr_b)
    backward_q = backward_q_array.stack()  # shape: [L_b+1, batch, d]
    backward_p = backward_p_array.stack()    # shape: [L_b+1, batch, d]

    # Reverse the backward trajectory (excluding the duplicate initial state q0)
    if L_b > 0:
        backward_q_ex = backward_q[1:]  # shape: [L_b, batch, d]
        backward_p_ex = backward_p[1:]
        backward_q_rev = tf.reverse(backward_q_ex, axis=[0])
        backward_p_rev = tf.reverse(backward_p_ex, axis=[0])
    else:
        backward_q_rev = tf.zeros([0, batch, d], dtype=q0.dtype)
        backward_p_rev = tf.zeros([0, batch, d], dtype=p0.dtype)
    # Concatenate backward (reversed) with forward trajectory to form full trajectory of length L+1.
    traj_q = tf.concat([backward_q_rev, forward_q], axis=0)
    traj_p = tf.concat([backward_p_rev, forward_p], axis=0)
    return traj_q, traj_p

class UnbiasedMultinomialHMC(tfp.mcmc.TransitionKernel):
    """
    UnbiasedMultinomialHMC implements a coupled multinomial HMC kernel.

    Update mechanism:
      - At each step, the kernel chooses between a coupled multinomial HMC update and a coupled MH update.
      - When a coupled multinomial HMC update is chosen, a full trajectory is simulated as detailed in simulate_trajectory().
      - When the current time step t is less than lag, only the X-chain is updated (the Y-chain remains unchanged).
      - The coupled MH update is performed by calling the external MH kernel (coupledRWMHKernel) using the tuple (X, Y).
    """
    def __init__(self,
                 target_log_prob_fn,
                 step_size,
                 L,  # Maximum leapfrog steps
                 momentum_distribution,
                 tolerance,
                 mix_prob,
                 lag,  # Number of initial steps during which Y is not updated
                 coupling_method="maximal",  # Options: "maximal" or "w2"
                 reg=1e-1,
                 proposal_var=None,  # Proposal variance for the MH update
                 seed=None,
                 name="UnbiasedMultinomialHMC"):
        """
        Initialize the kernel.

        Args:
            target_log_prob_fn: Callable mapping [batch, d] to log probability.
            step_size: Float leapfrog step size.
            L: Integer maximum leapfrog steps.
            momentum_distribution: A tfp.distributions instance for sampling momentum.
            tolerance: Float threshold for meeting.
            mix_prob: Float in [0,1] probability of performing a coupled MH update.
            lag: Integer, number of initial steps in which the Y-chain is not updated.
            coupling_method: String, either "maximal" or "w2" for intra-trajectory coupling.
            reg: Regularization parameter for W2 coupling.
            proposal_var: Float proposal variance for the MH update.
            seed: Optional integer random seed.
            name: Kernel name.
        """
        self._target_log_prob_fn = target_log_prob_fn
        self._step_size = step_size
        self._L = L
        self._momentum_distribution = momentum_distribution
        self._tolerance = tolerance
        self._mix_prob = mix_prob
        self._lag = lag
        self._coupling_method = coupling_method
        self._reg = reg
        self._seed = seed if seed is not None else 42
        self._name = name

        if self._mix_prob > 0:
            proposal_var = proposal_var if proposal_var is not None else 1.0
            # Use the externally defined coupled MH kernel.
            self._mh_kernel = CoupledMetropolisHastingsKernel(
                self._target_log_prob_fn,
                proposal_var,
                coupling_method="maximal",
                max_iter=5,
                seed=self._seed)
        else:
            self._mh_kernel = None

    @property
    def is_calibrated(self):
        return True

    def bootstrap_results(self, current_state):
        """
        Bootstraps kernel results.

        Args:
            current_state: Tuple (X, Y) of current states, each of shape [batch, d].

        Returns:
            A dictionary containing:
              - "target_log_prob": (logp_X, logp_Y)
              - "accepted": (False, False) tensors.
              - "proposals": (zeros, zeros) of the same shape as X and Y.
              - "current_log_prob": (logp_X, logp_Y)
              - "t": A scalar integer iteration counter.
              - "meeting_time": A tensor of shape [batch] initialized to -1 if chains have not met.
        """
        X, Y = current_state
        logp_X = self._target_log_prob_fn(X)
        logp_Y = self._target_log_prob_fn(Y)
        tol = tf.cast(self._tolerance, X.dtype)
        met_initial = tf.reduce_all(tf.abs(X - Y) < tol, axis=1)
        batch_size = tf.shape(X)[0]
        meeting_time = tf.where(met_initial,
                                tf.zeros([batch_size], dtype=tf.int32),
                                -tf.ones([batch_size], dtype=tf.int32))
        return {
            "target_log_prob": (logp_X, logp_Y),
            "accepted": (tf.zeros(tf.shape(logp_X), dtype=tf.bool),
                         tf.zeros(tf.shape(logp_Y), dtype=tf.bool)),
            "proposals": (tf.zeros_like(X), tf.zeros_like(Y)),
            "current_log_prob": (logp_X, logp_Y),
            "t": tf.constant(0, dtype=tf.int32),
            "meeting_time": meeting_time
        }

    def one_step(self, current_state, previous_kernel_results):
        """
        Performs one coupled update step.

        At each step, a coin toss (using a stateless seed) chooses between:
          - A coupled HMC update (simulating a full multinomial HMC trajectory with shared momentum
            and using the chosen intra-trajectory coupling strategy), and
          - A coupled MH update using the external MH kernel.
        In addition, if the current time t is less than lag, the Y-chain is not updated (remains as in current_state).

        Args:
            current_state: Tuple (X, Y) of current states, each of shape [batch, d].
            previous_kernel_results: Dictionary containing previous kernel information (including "t" and "meeting_time").

        Returns:
            A tuple (new_state, new_kernel_results) with the updated state tuple and updated kernel results.
        """
        X, Y = current_state
        t_old = previous_kernel_results["t"]
        new_t = t_old + 1
        coin_seed = tf.stack([tf.cast(self._seed, tf.int32), new_t])
        coin = tf.random.stateless_uniform([], seed=coin_seed, minval=0, maxval=1, dtype=tf.float32)

        def hmc_update():
            batch = tf.shape(X)[0]
            # Sample shared momentum for both chains.
            p = self._momentum_distribution.sample(sample_shape=[batch], seed=coin_seed)
            traj_X, traj_PX = simulate_trajectory(X, p, self._L, self._step_size, self._target_log_prob_fn)
            traj_Y, traj_PY = simulate_trajectory(Y, p, self._L, self._step_size, self._target_log_prob_fn)
            T = tf.shape(traj_X)[0]  # T = self._L + 1
            # Compute log probabilities along the trajectory.
            flat_traj_X = tf.reshape(traj_X, [T * batch, -1])
            flat_traj_Y = tf.reshape(traj_Y, [T * batch, -1])
            logp_X_all = self._target_log_prob_fn(flat_traj_X)
            logp_Y_all = self._target_log_prob_fn(flat_traj_Y)
            logp_X_all = tf.reshape(logp_X_all, [T, batch])
            logp_Y_all = tf.reshape(logp_Y_all, [T, batch])
            # Compute kinetic energies as 0.5 * ||p||^2 at each trajectory point.
            kinetic_X = 0.5 * tf.reduce_sum(tf.square(traj_PX), axis=-1)
            kinetic_Y = 0.5 * tf.reduce_sum(tf.square(traj_PY), axis=-1)
            energy_X = -logp_X_all + kinetic_X  # Shape [T, batch]
            energy_Y = -logp_Y_all + kinetic_Y
            weights_X = tf.exp(-energy_X)
            weights_Y = tf.exp(-energy_Y)
            # Normalize weights along the trajectory dimension for each batch element.
            norm_weights_X = weights_X / tf.reduce_sum(weights_X, axis=0, keepdims=True)
            norm_weights_Y = weights_Y / tf.reduce_sum(weights_Y, axis=0, keepdims=True)
            # Transpose to shape [batch, T] for coupling functions.
            norm_weights_X_t = tf.transpose(norm_weights_X)
            norm_weights_Y_t = tf.transpose(norm_weights_Y)
            if self._coupling_method == "maximal":
                from coupling_algorithms import maximal_coupling as maximal_multinomial_coupling
                idx1, idx2 = maximal_multinomial_coupling(norm_weights_X_t, norm_weights_Y_t)
            else:
                # For W2 coupling, construct cost matrix based on squared differences between indices.
                T_float = tf.cast(T, tf.float32)
                indices = tf.cast(tf.range(T), tf.float32)
                cost_matrix = tf.square(tf.expand_dims(indices, 1) - tf.expand_dims(indices, 0))
                from coupling_algorithms import w2_multinomial_coupling
                idx1, idx2 = w2_multinomial_coupling(norm_weights_X_t, norm_weights_Y_t, cost_matrix, reg=self._reg)
            new_X = tf.gather(traj_X, idx1, axis=0, batch_dims=1)
            new_Y = tf.gather(traj_Y, idx2, axis=0, batch_dims=1)
            return (new_X, new_Y)

        def mh_update():
            # Call the external MH kernel with the state tuple (X, Y) and corresponding bootstrap results.
            new_state, _ = self._mh_kernel.one_step((X, Y),
                                                     self._mh_kernel.bootstrap_results((X, Y)),
                                                     seed=coin_seed)
            return new_state

        use_mh = tf.less(coin, self._mix_prob) if self._mh_kernel is not None else tf.constant(False)
        new_state = tf.cond(use_mh, lambda: mh_update(), lambda: hmc_update())
        # For steps with time t < lag, do not update Y (keep Y unchanged).
        new_state = (new_state[0], tf.cond(tf.less(t_old, self._lag),
                                             lambda: Y,
                                             lambda: new_state[1]))
        tol = tf.cast(self._tolerance, new_state[0].dtype)
        diff = tf.reduce_max(tf.abs(new_state[0] - new_state[1]), axis=-1)
        prev_mt = previous_kernel_results["meeting_time"]
        new_mt = tf.cond(tf.less(t_old, self._lag),
                         lambda: prev_mt,
                         lambda: tf.where((prev_mt < 0) & (diff < tol),
                                            tf.fill(tf.shape(prev_mt), new_t),
                                            prev_mt))
        new_kernel_results = {
            "target_log_prob": (self._target_log_prob_fn(new_state[0]),
                                self._target_log_prob_fn(new_state[1])),
            "accepted": (tf.constant(False), tf.constant(False)),
            "proposals": (tf.zeros_like(new_state[0]), tf.zeros_like(new_state[1])),
            "current_log_prob": (self._target_log_prob_fn(new_state[0]),
                                 self._target_log_prob_fn(new_state[1])),
            "t": new_t,
            "meeting_time": new_mt
        }
        return new_state, new_kernel_results

# End of module.
